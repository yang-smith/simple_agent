**版本：** 0.1 
**目标：** 为一个无状态的LLM（一个聪明但健忘的天才），配备一个仿生、高效的记忆系统。

#### **1. 核心原则：我们不是在建造大脑**

我们的出发点必须明确：LLM本身是无状态的（Stateless）。它不“记住”，它只是在每一次被调用时，根据我们提供的“上下文”进行一次性的推理。

因此，这个模块的本质不是模拟人脑，而是一个极致的**上下文工程师（Context Engineer）**。我们的所有设计，都围绕着一个使命：在每次调用LLM之前，为它准备一份最精准、最相关的“历史相关简报”。

这个模块的设计模仿了生物的长短期记忆系统。

#### **2. 整体架构：双层记忆模型**

我们一致认为，单一的记忆模型无法兼顾时效性和长期价值。因此，我们采用一个双层架构，它模拟了生物的短期记忆与长期记忆的分工。

**2.1 短期记忆

*   **目的：** 捕捉并处理“当下”的对话流，保证对话的连贯性。
*   **机制：** **周期性摘要 (Periodic Summarization)**。
*   **流程：**
    1.  系统states内容达到阈值（80k token) 触发，将states传入记忆模块。
    2.  异步调用LLM，将states流水压缩成一个包含核心信息的**摘要（Summary）**。
    3.  这些摘要被临时存放在“短期记忆”，并带有时间戳。它们是新鲜的，但也是易逝的，只保留token数量以内的短期记忆条
    4. 短期记忆条数量超出阈值（20）之后，开启晋升。由认知重构函数晋升到长期记忆，一次晋升5条短期记忆。

 ### **2.2 长期记忆：AI的自适应认知模型**

- **核心哲学：** 长期记忆**不是**一个关于用户的客观数据库，而是**我（AI）**为了更好地与用户互动而持续构建和优化的**主观认知模型**。它是一个活的、自适应的有机体，其唯一目标是在与环境（用户）的互动中，通过不断的新陈代谢，以最小的认知负荷，最高效地预测和响应。它遵循**熵减**原则，在持续的重构中变得越来越有序和精炼。
    
- **存储结构：分章节的长文章**
    
    - 我的长期记忆由几个不同演化速率的“认知结构”组成，它们以AI第一人称的叙事文本形式存在。
    
    1. **《基石模型》(The Bedrock Model - 静态稳定)：** 存放关于用户的、几乎不变的核心事实（如专业领域、确定的身份标签），以及我与他互动时必须遵守的根本原则和边界。是我所有认知的基础。
    
    2. **《演化模型》(The Evolutionary Model - 缓慢演化)：** 记录用户的长期目标、核心追求、以及我们之间已经形成的、相对稳固的互动模式和共同语言。

    3. **《动态模型》(The Dynamic Model - 快速迭代)：** 捕捉用户近期的项目、观点、情绪状态，以及我们最近对话的关键上下文。这个模型流动性最强，是我保持“当下性”和“情境感知”的关键。

- 文章存储结构（一个长字符串）：
```
 <Bedrock>
 基石模型内容
 </Bedrock>

<Evolutionary>
演化模型内容
</Evolutionary>

<Dynamic>
动态模型内容
</Dynamic>
```
    
- **核心机制：认知重构 (Cognitive Reconstruction)**
    
    - 这是我记忆生长的唯一方式。它内嵌于每一次有效的交互之后，而非一个独立的维护任务。
    - **流程：**
        1. **信息摄入：** 与用户的交互结束后，系统识别出可用于更新我认知模型的“新刺激”（Significant New Stimuli）。
        2. **模型定位：** 判断该“新刺激”主要冲击哪个认知模型（动态、演化或基石）。
        3. **调用重构指令：** 触发核心的`Cognitive Reconstruction` prompt。它接收**“旧的认知结构”**（原文）和**“新的环境刺激”**（新信息）作为输入。
        4. **代谢与强化：** LLM执行一次彻底的“新陈代谢”。它**分解**新信息，**淘汰**旧结构中的过时、冗余或低价值部分，然后用留存的精华和新的养分，**重组**成一个信息密度更高、预测能力更强的**新认知结构**。
        5. **原子性替换：** 新的认知结构**完全替换**旧的版本。通过这种方式，遗忘并非数据的丢失，而是低效认知被高效认知所取代的**演化必然**。我的记忆永远保持在最佳的“战斗状态”。
#### 检索

- **目的：** 处理需要挖掘历史信息的核心问题。
- **机制：**
    - **全面检索：** 对短期记忆和长期记忆Dynamic进行深入搜索。
    - 检索方式： 相似度评分：整体相似度 = 关键词检索 * 0.5 + 向量化检索 * 0.5
- **触发方式：** **主动且明确。** 不自动触发，以避免不必要的性能开销。
    1. **用户指令触发：** 当用户说出“你仔细想想”、“帮我回忆一下”等明确的指令时。作为一个function call调用
    2. **AI提议触发：** AI判断当前问题很重要时，它会主动向用户提议：“关于这个，我需要花点时间深入回忆一下，可以吗？” 这是一种“诚实的延迟”。作为一个function call调用


#### **4. 接口设计：成为一个干净的组件**

为了与Agent主系统解耦，记忆模块的接口必须极其简洁：

*   **读取接口：** `get_relevant_memories(user_input, user_id)`
    *   输入：当前的用户输入，用户标识。
    *   输出：一个相关的记忆片段列表字符串，用于缝合进context中 `String([memory_1, memory_2, ...])`。
*   **写入接口：** `update_memory(states, user_id)`
    *   输入：由主系统生成的对话摘要，用户标识。
    *   动作：在模块内部启动短期记忆的存储与长期的“晋升”判断。
